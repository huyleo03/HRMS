import React, { useState, useRef, useEffect } from 'react';
import { Camera, X, CheckCircle, AlertCircle, Loader } from 'lucide-react';
import { toast } from 'react-toastify';
import FaceRecognitionService from '../../service/FaceRecognitionService';
import { apiCall } from '../../service/api';
import './FaceIdEnrollment.css';
import './FaceIdQuickVerification.css';

/**
 * üéØ FACE ID QUICK VERIFICATION
 * Ch·ª•p 1 l·∫ßn, so s√°nh v·ªõi 5 g√≥c ƒë√£ l∆∞u
 */

const FaceIdQuickVerification = ({ actionType, onSuccess, onCancel }) => {
  // Random ch·ªçn 1 h∆∞·ªõng NGAY T·ª™ ƒê·∫¶U
  const movements = ['left', 'right', 'up', 'down'];
  const initialMovement = movements[Math.floor(Math.random() * movements.length)];
  
  const [step, setStep] = useState('ready'); // ready, detecting, capturing, processing, success, error
  const [errorMessage, setErrorMessage] = useState('');
  const [capturedImage, setCapturedImage] = useState(null);
  const [isVerifying, setIsVerifying] = useState(false);
  const [requiredMovement, setRequiredMovement] = useState(initialMovement); // Set ngay t·ª´ ƒë·∫ßu
  const [movementProgress, setMovementProgress] = useState(0);

  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const streamRef = useRef(null);
  const detectionIntervalRef = useRef(null);
  const lastDetectionRef = useRef(null);
  const hasCalledSuccessRef = useRef(false); // üîí NgƒÉn g·ªçi onSuccess nhi·ªÅu l·∫ßn

  useEffect(() => {
    console.log(`üéØ Required movement: ${requiredMovement}`);
    startCamera();

    return () => {
      stopCamera();
    };
  }, []);

  const startCamera = async () => {
    try {
      console.log('üìπ Requesting camera access...');
      const stream = await navigator.mediaDevices.getUserMedia({
        video: {
          width: { ideal: 1280 },
          height: { ideal: 720 },
          facingMode: 'user',
        },
      });

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        streamRef.current = stream;
        
        // ƒê·ª£i video load xong r·ªìi b·∫Øt ƒë·∫ßu ph√°t hi·ªán chuy·ªÉn ƒë·ªông
        videoRef.current.onloadedmetadata = () => {
          console.log('üìπ Camera loaded, starting movement detection...');
          startMovementDetection();
        };
      }
    } catch (error) {
      console.error('‚ùå Camera error:', error);
      toast.error('Kh√¥ng th·ªÉ truy c·∫≠p camera!');
      setStep('error');
      setErrorMessage('Kh√¥ng th·ªÉ truy c·∫≠p camera');
    }
  };

  const stopCamera = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (detectionIntervalRef.current) {
      clearInterval(detectionIntervalRef.current);
    }
  };

  /**
   * B·∫Øt ƒë·∫ßu ph√°t hi·ªán chuy·ªÉn ƒë·ªông li√™n t·ª•c
   */
  const startMovementDetection = () => {
    console.log('üé¨ Starting movement detection...');
    setStep('detecting');
    
    // Ph√°t hi·ªán m·ªói 200ms
    detectionIntervalRef.current = setInterval(() => {
      detectMovement();
    }, 200);
  };

  /**
   * Ph√°t hi·ªán chuy·ªÉn ƒë·ªông c·ªßa ƒë·∫ßu
   */
  const detectMovement = async () => {
    // Kh√¥ng check step n·ªØa v√¨ interval ch·ªâ ch·∫°y khi detecting
    // if (step !== 'detecting') return;

    try {
      const video = videoRef.current;
      const canvas = canvasRef.current;

      if (!video || !canvas) {
        console.log('‚ö†Ô∏è Video or canvas not ready');
        return;
      }

      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);

      const imageData = canvas.toDataURL('image/jpeg', 0.8);
      const detection = await FaceRecognitionService.detectFace(imageData);

      if (!detection) {
        setMovementProgress(0);
        console.log('‚ö†Ô∏è No face detected');
        return;
      }

      // L·∫•y landmarks ƒë·ªÉ t√≠nh g√≥c
      const landmarks = detection.landmarks;
      const nose = landmarks.getNose();
      const leftEye = landmarks.getLeftEye();
      const rightEye = landmarks.getRightEye();
      const mouth = landmarks.getMouth();

      // T√≠nh trung ƒëi·ªÉm 2 m·∫Øt
      const eyeCenter = {
        x: (leftEye[0].x + rightEye[3].x) / 2,
        y: (leftEye[0].y + rightEye[3].y) / 2,
      };

      const nosePoint = nose[3]; // Tip of nose
      const mouthCenter = {
        x: (mouth[0].x + mouth[6].x) / 2,
        y: (mouth[3].y + mouth[9].y) / 2,
      };

      // T√≠nh kho·∫£ng c√°ch gi·ªØa 2 m·∫Øt ƒë·ªÉ l√†m t·ª∑ l·ªá
      const eyeDistance = Math.sqrt(
        Math.pow(rightEye[3].x - leftEye[0].x, 2) + 
        Math.pow(rightEye[3].y - leftEye[0].y, 2)
      );

      // T√≠nh g√≥c yaw (xoay tr√°i/ph·∫£i)
      const dx = nosePoint.x - eyeCenter.x;
      const yaw = Math.atan2(dx, eyeDistance * 1.5) * (180 / Math.PI);
      
      // T√≠nh pitch (ng·∫©ng/c√∫i)
      const noseTipY = nosePoint.y;
      const eyeCenterY = eyeCenter.y;
      const mouthBottomY = mouth[9].y;
      const pitchRatio = (noseTipY - eyeCenterY) / (mouthBottomY - eyeCenterY);
      const pitch = pitchRatio * 100;

      // Ki·ªÉm tra chuy·ªÉn ƒë·ªông theo h∆∞·ªõng y√™u c·∫ßu
      let isCorrectMovement = false;
      const YAW_THRESHOLD = 5; // Gi·∫£m t·ª´ 8 xu·ªëng 5 - d·ªÖ h∆°n
      const PITCH_UP_THRESHOLD = 40; // TƒÉng t·ª´ 35 l√™n 40 - d·ªÖ h∆°n
      const PITCH_DOWN_THRESHOLD = 50; // Gi·∫£m t·ª´ 55 xu·ªëng 50 - d·ªÖ h∆°n

      switch (requiredMovement) {
        case 'left':
          isCorrectMovement = yaw < -YAW_THRESHOLD;
          break;
        case 'right':
          isCorrectMovement = yaw > YAW_THRESHOLD;
          break;
        case 'up':
          isCorrectMovement = pitch < PITCH_UP_THRESHOLD;
          break;
        case 'down':
          isCorrectMovement = pitch > PITCH_DOWN_THRESHOLD;
          break;
      }

      // DEBUG: Log movement data
      console.log(`üéØ Movement Detection - Required: ${requiredMovement}, Yaw: ${yaw.toFixed(1)}¬∞, Pitch: ${pitch.toFixed(1)}, Match: ${isCorrectMovement}`);

      if (isCorrectMovement) {
        // TƒÉng progress nhanh h∆°n
        setMovementProgress(prev => {
          const newProgress = Math.min(prev + 25, 100); // TƒÉng t·ª´ 20 l√™n 25 - nhanh h∆°n
          console.log(`‚úÖ Correct movement! Progress: ${prev}% ‚Üí ${newProgress}%`);
          
          // Khi ƒë·∫°t 100% ‚Üí T·ª± ƒë·ªông ch·ª•p (CH·ªà 1 L·∫¶N)
          if (newProgress >= 100 && prev < 100) { // ‚úÖ Ki·ªÉm tra prev < 100 ƒë·ªÉ ch·ªâ g·ªçi 1 l·∫ßn
            console.log('üì∏ Progress reached 100%, capturing...');
            clearInterval(detectionIntervalRef.current);
            detectionIntervalRef.current = null; // Set null ƒë·ªÉ kh√¥ng g·ªçi l·∫°i
            setTimeout(() => captureAndVerify(), 100); // Delay nh·ªè ƒë·ªÉ UI c·∫≠p nh·∫≠t
          }
          
          return newProgress;
        });
      } else {
        // Gi·∫£m progress ch·∫≠m h∆°n
        setMovementProgress(prev => Math.max(prev - 5, 0)); // Gi·∫£m t·ª´ 10 xu·ªëng 5 - √≠t kh·∫Øt khe h∆°n
      }

      // L∆∞u detection ƒë·ªÉ debug
      lastDetectionRef.current = { yaw, pitch, isCorrectMovement };

    } catch (error) {
      console.error('Movement detection error:', error);
    }
  };

  const captureAndVerify = async () => {
    if (isVerifying) return;

    console.log('üì∏ Starting capture and verify...');
    
    // Stop movement detection
    if (detectionIntervalRef.current) {
      clearInterval(detectionIntervalRef.current);
      detectionIntervalRef.current = null;
    }

    setIsVerifying(true);
    setStep('capturing');

    try {
      const video = videoRef.current;
      const canvas = canvasRef.current;

      canvas.width = video.videoWidth;
      canvas.height = video.videoHeight;
      canvas.getContext('2d').drawImage(video, 0, 0);

      const imageData = canvas.toDataURL('image/jpeg', 0.95);
      setCapturedImage(imageData);

      // Ph√°t hi·ªán khu√¥n m·∫∑t
      setStep('processing');
      const detection = await FaceRecognitionService.detectFace(imageData);

      if (!detection) {
        throw new Error('Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t! Vui l√≤ng th·ª≠ l·∫°i.');
      }

      // L·∫•y descriptor t·ª´ ·∫£nh ch·ª•p
      const descriptor = Array.from(detection.descriptor);
      
      console.log('üì§ Verifying Face ID after movement detection...');
      
      // G·ªçi API verify - s·∫Ω so s√°nh v·ªõi T·∫§T C·∫¢ 5 descriptors ƒë√£ l∆∞u
      const response = await apiCall('/api/face-id/verify', {
        method: 'POST',
        body: JSON.stringify({
          descriptor: descriptor,
          threshold: 0.6, // Ng∆∞·ª°ng t∆∞∆°ng ƒë·ªìng (60%)
        }),
      });

      console.log('‚úÖ Verification response:', response);

      if (response.success && response.data.verified) {
        setStep('success');
        toast.success(`‚úÖ X√°c th·ª±c th√†nh c√¥ng! ƒê·ªô t∆∞∆°ng ƒë·ªìng: ${response.data.similarity}%`, {
          toastId: 'face-verification-success' // ‚úÖ Ch·ªâ hi·ªÉn th·ªã 1 toast
        });
        
        // ‚úÖ Ch·ªâ g·ªçi onSuccess M·ªòT L·∫¶N duy nh·∫•t
        if (!hasCalledSuccessRef.current) {
          hasCalledSuccessRef.current = true;
          setTimeout(() => {
            stopCamera();
            onSuccess && onSuccess(response.data);
          }, 1500);
        }
      } else {
        throw new Error(response.message || `X√°c th·ª±c th·∫•t b·∫°i. ƒê·ªô t∆∞∆°ng ƒë·ªìng: ${response.data?.similarity || 0}%`);
      }
    } catch (error) {
      console.error('‚ùå Verification error:', error);
      setStep('error');
      setErrorMessage(error.message || 'L·ªói khi x√°c th·ª±c Face ID');
      toast.error(error.message || 'L·ªói khi x√°c th·ª±c Face ID');
      
      // Auto retry after 2s
      setTimeout(() => {
        setStep('ready');
        setErrorMessage('');
        setCapturedImage(null);
      }, 2000);
    } finally {
      setIsVerifying(false);
    }
  };

  const handleCancel = () => {
    stopCamera();
    onCancel && onCancel();
  };

  const getMovementInstruction = () => {
    const instructions = {
      left: 'üëà Vui l√≤ng NGHI√äNG ƒê·∫¶U SANG TR√ÅI',
      right: 'üëâ Vui l√≤ng NGHI√äNG ƒê·∫¶U SANG PH·∫¢I',
      up: 'üëÜ Vui l√≤ng NG·∫®NG ƒê·∫¶U L√äN',
      down: 'üëá Vui l√≤ng C√öI ƒê·∫¶U XU·ªêNG'
    };
    return instructions[requiredMovement] || '';
  };

  const getMovementIcon = () => {
    const icons = {
      left: 'üëà',
      right: 'üëâ',
      up: 'üëÜ',
      down: 'üëá'
    };
    return icons[requiredMovement] || 'üì∑';
  };

  // ===== RENDER =====

  return (
    <div className="face-id-enrollment">
      <div className="enrollment-scanner">
        <div className="scanner-header">
          <h2>
            {actionType === 'in' ? 'üì• Check-in v·ªõi Face ID' : 'üì§ Check-out v·ªõi Face ID'}
          </h2>
          <p>
            {step === 'ready' && 'ƒêang kh·ªüi ƒë·ªông camera...'}
            {step === 'detecting' && getMovementInstruction()}
            {step === 'capturing' && 'ƒêang ch·ª•p ·∫£nh...'}
            {step === 'processing' && 'ƒêang x√°c th·ª±c khu√¥n m·∫∑t...'}
            {step === 'success' && '‚úÖ X√°c th·ª±c th√†nh c√¥ng!'}
            {step === 'error' && `‚ùå ${errorMessage}`}
          </p>
          
          {/* Progress Bar */}
          {step === 'detecting' && (
            <div className="movement-progress">
              <div className="progress-bar-container">
                <div 
                  className="progress-bar-fill" 
                  style={{ width: `${movementProgress}%` }}
                />
              </div>
              <span className="progress-text">{movementProgress}%</span>
            </div>
          )}
        </div>

        <div className="scanner-video">
          <video ref={videoRef} autoPlay playsInline />
          <canvas ref={canvasRef} style={{ display: 'none' }} />
          
          {capturedImage && (
            <div className="captured-overlay">
              <img src={capturedImage} alt="Captured" />
            </div>
          )}

          {/* Face Guide Overlay */}
          {step === 'detecting' && (
            <div className="face-guide-simple">
              <div className="face-circle"></div>
              <div className="movement-indicator">
                <span className="movement-icon">{getMovementIcon()}</span>
                <p className="movement-text">{getMovementInstruction()}</p>
              </div>
            </div>
          )}

          {step === 'ready' && (
            <div className="face-guide-simple">
              <div className="face-circle"></div>
              <p className="guide-text">ƒêang kh·ªüi ƒë·ªông...</p>
            </div>
          )}

          {/* Processing Overlay */}
          {step === 'processing' && (
            <div className="processing-overlay">
              <Loader className="spin-animation" size={48} />
              <p>ƒêang x√°c th·ª±c...</p>
            </div>
          )}

          {/* Success Overlay */}
          {step === 'success' && (
            <div className="success-overlay">
              <CheckCircle size={64} color="#10B981" />
              <p>X√°c th·ª±c th√†nh c√¥ng!</p>
            </div>
          )}

          {/* Error Overlay */}
          {step === 'error' && (
            <div className="error-overlay">
              <AlertCircle size={64} color="#EF4444" />
              <p>{errorMessage}</p>
              <small>Th·ª≠ l·∫°i sau 2 gi√¢y...</small>
            </div>
          )}
        </div>

        <div className="scanner-actions">
          <button 
            onClick={handleCancel} 
            className="btn-cancel-floating"
            disabled={isVerifying || step === 'success'}
          >
            <X size={20} />
            <span>H·ªßy</span>
          </button>
        </div>
      </div>
    </div>
  );
};

export default FaceIdQuickVerification;
